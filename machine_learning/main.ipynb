{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classifications import Classifications\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = Classifications()\n",
    "model_h_path = \"../ModelData.h\"\n",
    "scaler_h_path = \"../Scaler.h\"\n",
    "best_model_path = \"best_model.keras\"\n",
    "data_csvs = glob.glob(\"csvs/*.csv\")\n",
    "batch_size = 128\n",
    "num_epochs = 512\n",
    "patience = 256\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "test_split = 0.2\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.InputLayer((classifications.num_steps, classifications.num_features), dtype=tf.float32),\n",
    "\n",
    "    layers.Conv1D(32, 3, padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "\n",
    "    layers.Conv1D(64, 3, padding=\"same\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation(\"relu\"),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Dropout(0.1),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dropout(0.4),\n",
    "    layers.Dense(classifications.num_classes, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.random.uniform(shape=(1, classifications.num_steps, classifications.num_features), dtype=tf.float32, seed=seed)\n",
    "logits = model.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(logits, axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_string = ' '.join(reversed(classifications.classes[prediction]))\n",
    "print(f\"Prediction: {prediction_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(data_csv) for data_csv in data_csvs], ignore_index=True)\n",
    "\n",
    "# Peak detection\n",
    "peaks, _ = find_peaks(\n",
    "    np.sum(df[[\"ax\", \"ay\", \"az\"]].values ** 2, axis=1),\n",
    "    height=classifications.squared_acceleration_threshold,\n",
    "    distance=classifications.num_steps,\n",
    ")\n",
    "\n",
    "sensor_columns = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\"]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for peak in peaks:\n",
    "    start_idx = peak - classifications.steps_before_peak\n",
    "    end_idx = peak + classifications.steps_after_peak\n",
    "    if start_idx < 0 or end_idx >= len(df):\n",
    "        continue\n",
    "\n",
    "    shot_df = df.loc[start_idx:end_idx]\n",
    "    assert len(shot_df) == classifications.num_steps\n",
    "\n",
    "    shot_data = shot_df[sensor_columns].values\n",
    "    stroke = df.loc[peak, \"stroke\"].lower()\n",
    "    side = df.loc[peak, \"side\"].lower()\n",
    "    spin = df.loc[peak, \"spin\"].lower()\n",
    "    label_key = (stroke, side, spin)\n",
    "\n",
    "    if label_key in classifications.classes:\n",
    "        label = classifications.class_to_idx[label_key]\n",
    "        X.append(shot_data)\n",
    "        y.append(label)\n",
    "\n",
    "X = np.array(X).astype(np.float32)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, random_state=seed, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_split, random_state=seed, stratify=y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "\n",
    "with open(scaler_h_path, \"w\") as f:\n",
    "    f.write(\"#ifndef _SCALER_H_\\n#define _SCALER_H_\\n\\n#include \\\"SensorData.h\\\"\\n\\n\")\n",
    "    f.write(\"constexpr float mean_[NUM_FEATURES] = {\")\n",
    "    f.write(\",\".join(f\"{x}\" for x in scaler.mean_))\n",
    "    f.write(\"};\\n\")\n",
    "    f.write(\"constexpr float scale_[NUM_FEATURES] = {\")\n",
    "    f.write(\",\".join(f\"{x}\" for x in scaler.scale_))\n",
    "    f.write(\"};\\n\")\n",
    "    f.write(\"\\n#endif\\n\")\n",
    "\n",
    "def transform_data(X, scaler):\n",
    "    num_samples, num_steps, num_features = X.shape\n",
    "    X_flat = X.reshape(-1, num_features)\n",
    "    X_scaled_flat = scaler.transform(X_flat)\n",
    "    return X_scaled_flat.reshape(num_samples, num_steps, num_features)\n",
    "\n",
    "X_train = transform_data(X_train, scaler)\n",
    "X_val = transform_data(X_val, scaler)\n",
    "X_test = transform_data(X_test, scaler)\n",
    "\n",
    "def random_rotate_sample_tf(sample, label):\n",
    "    def random_rotation_matrix():\n",
    "        u1 = tf.random.uniform([], 0, 1)\n",
    "        u2 = tf.random.uniform([], 0, 1)\n",
    "        u3 = tf.random.uniform([], 0, 1)\n",
    "\n",
    "        q1 = tf.sqrt(1 - u1) * tf.sin(2 * np.pi * u2)\n",
    "        q2 = tf.sqrt(1 - u1) * tf.cos(2 * np.pi * u2)\n",
    "        q3 = tf.sqrt(u1) * tf.sin(2 * np.pi * u3)\n",
    "        q4 = tf.sqrt(u1) * tf.cos(2 * np.pi * u3)\n",
    "\n",
    "        x, y, z, w = q1, q2, q3, q4\n",
    "        rot = tf.stack([\n",
    "            [1 - 2*y*y - 2*z*z,     2*x*y - 2*z*w,     2*x*z + 2*y*w],\n",
    "            [2*x*y + 2*z*w,     1 - 2*x*x - 2*z*z,     2*y*z - 2*x*w],\n",
    "            [2*x*z - 2*y*w,         2*y*z + 2*x*w, 1 - 2*x*x - 2*y*y]\n",
    "        ])\n",
    "        return rot\n",
    "\n",
    "    R = random_rotation_matrix()\n",
    "    R = tf.cast(R, sample.dtype)\n",
    "\n",
    "    accel = sample[:, :3]\n",
    "    gyro = sample[:, 3:]\n",
    "\n",
    "    rotated_accel = tf.linalg.matmul(accel, R, transpose_b=True)\n",
    "    rotated_gyro = tf.linalg.matmul(gyro, R, transpose_b=True)\n",
    "\n",
    "    rotated_sample = tf.concat([rotated_accel, rotated_gyro], axis=1)\n",
    "    return rotated_sample, label\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train), seed=seed)\n",
    "    .map(random_rotate_sample_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=best_model_path,\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = {i: class_weights[np.where(np.unique(y_train) == i)[0][0]] if i in np.unique(y_train) else 0 for i in range(classifications.num_classes)}\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    validation_data=val_ds,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Non-Quantized Test Loss: {test_loss}\")\n",
    "print(f\"Non-Quantized Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f\"best_model.keras\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "def representative_dataset():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(X_train.astype(np.float32)).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type  = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "input_scale, input_zero_point = input_details['quantization']\n",
    "output_scale, output_zero_point = output_details['quantization']\n",
    "\n",
    "# Evaluate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for x, y_true in test_ds:\n",
    "    for i in range(x.shape[0]):\n",
    "        x_input = x[i].numpy()\n",
    "        x_input = np.round(x_input / input_scale + input_zero_point).astype(input_details[\"dtype\"])\n",
    "        x_input = np.expand_dims(x_input, axis=0)\n",
    "\n",
    "        interpreter.set_tensor(input_details['index'], x_input)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        output = interpreter.get_tensor(output_details['index'])[0]\n",
    "        y_pred = np.argmax(output)\n",
    "\n",
    "        if y_pred == y_true[i].numpy():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Quantized Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model size: {len(tflite_model)} bytes\")\n",
    "\n",
    "with open(model_h_path, \"w\") as f:\n",
    "    f.write(\"#ifndef _MODELDATA_H_\\n#define _MODELDATA_H_\\n\")\n",
    "    f.write(\"const unsigned char model[] = {\")\n",
    "    f.write(\",\".join(f\"0x{b:02x}\" for b in tflite_model))\n",
    "    f.write(\"};\\n#endif\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
