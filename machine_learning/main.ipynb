{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classifications import Classifications\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import InputLayer, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = Classifications()\n",
    "model_h_path = \"../ModelData.h\"\n",
    "data_csv = \"sensor_data_cleaned.csv\"\n",
    "batch_size = 128\n",
    "num_epochs = 128\n",
    "patience = 16\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "test_split = 0.2\n",
    "val_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    InputLayer((classifications.num_steps, classifications.num_features)),\n",
    "    Conv1D(16, 3, activation=\"relu\"),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(32, 3, activation=\"relu\"),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(classifications.num_classes, activation=None),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = tf.random.uniform(shape=(1, classifications.num_steps, classifications.num_features), dtype=tf.float16, seed=seed)\n",
    "logits = model.predict(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.argmax(logits, axis=1)[0]\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_string = ' '.join(reversed(classifications.classes[prediction]))\n",
    "print(f\"Prediction: {prediction_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_csv)\n",
    "df = pd.concat([df, df], ignore_index=True)\n",
    "\n",
    "# Peak detection\n",
    "peaks, _ = find_peaks(\n",
    "    np.sum(df[[\"ax\", \"ay\", \"az\"]].values ** 2, axis=1),\n",
    "    height=classifications.squared_acceleration_threshold,\n",
    "    distance=classifications.num_steps,\n",
    ")\n",
    "\n",
    "sensor_columns = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\"]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for peak in peaks:\n",
    "    start_idx = peak - classifications.steps_before_peak\n",
    "    end_idx = peak + classifications.steps_after_peak\n",
    "    if start_idx < 0 or end_idx >= len(df):\n",
    "        continue\n",
    "\n",
    "    shot_df = df.loc[start_idx:end_idx]\n",
    "    assert len(shot_df) == classifications.num_steps\n",
    "\n",
    "    shot_data = shot_df[sensor_columns].values\n",
    "    stroke = df.loc[peak, \"stroke\"].lower()\n",
    "    side = df.loc[peak, \"side\"].lower()\n",
    "    spin = df.loc[peak, \"spin\"].lower()\n",
    "    label_key = (stroke, side, spin)\n",
    "\n",
    "    label = classifications.class_to_idx[label_key]\n",
    "    X.append(shot_data)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X).astype(np.float16)\n",
    "y = np.array(y)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=test_split, random_state=seed, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_split, random_state=seed, stratify=y_temp)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "\n",
    "def transform_data(X, scaler):\n",
    "    num_samples, num_steps, num_features = X.shape\n",
    "    X_flat = X.reshape(-1, num_features)\n",
    "    X_scaled_flat = scaler.transform(X_flat)\n",
    "    return X_scaled_flat.reshape(num_samples, num_steps, num_features)\n",
    "\n",
    "X_train = transform_data(X_train, scaler)\n",
    "X_val = transform_data(X_val, scaler)\n",
    "X_test = transform_data(X_test, scaler)\n",
    "\n",
    "def random_rotate_sample(sample, label):\n",
    "    def rotate_fn(sample_np):\n",
    "        random_rotation = R.random(rng=rng)\n",
    "        rotated_accel = random_rotation.apply(sample_np[:, :3])\n",
    "        rotated_gyro = random_rotation.apply(sample_np[:, 3:])\n",
    "        rotated_sample = np.concatenate((rotated_accel, rotated_gyro), axis=1)\n",
    "        return rotated_sample.astype(np.float16)\n",
    "\n",
    "    rotated_sample = tf.py_function(func=rotate_fn, inp=[sample], Tout=tf.float16)\n",
    "    rotated_sample.set_shape(sample.shape)\n",
    "    return rotated_sample, label\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train), seed=seed)\n",
    "    .map(random_rotate_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#sample, label = next(iter(train_ds))\n",
    "#sample = sample[0]\n",
    "#label = label[0]\n",
    "#\n",
    "#plot_width = 6\n",
    "#x_ticks = np.arange(len(sample))\n",
    "#\n",
    "## Raw data plot\n",
    "#plots = (\n",
    "#    { \"label\": \"ax\", \"title\": \"Acceleration X\", \"color\": \"r\" },\n",
    "#    { \"label\": \"ay\", \"title\": \"Acceleration Y\", \"color\": \"g\" },\n",
    "#    { \"label\": \"az\", \"title\": \"Acceleration Z\", \"color\": \"b\" },\n",
    "#    { \"label\": \"gx\", \"title\": \"Gyroscope X\", \"color\": \"r\" },\n",
    "#    { \"label\": \"gy\", \"title\": \"Gyroscope Y\", \"color\": \"g\" },\n",
    "#    { \"label\": \"gz\", \"title\": \"Gyroscope Z\", \"color\": \"b\" },\n",
    "#)\n",
    "#\n",
    "#fig, axes = plt.subplots(len(plots), 1, figsize=(plot_width, 3 * len(plots)))\n",
    "#for i, (ax, plot) in enumerate(zip(axes, plots)):\n",
    "#    ax.plot(x_ticks, sample[:, i], label=plot[\"title\"], color=plot[\"color\"])\n",
    "#    ax.set_xlabel(\"Milliseconds\")\n",
    "#    ax.set_ylabel(plot[\"title\"])\n",
    "#    ax.set_xticks(x_ticks[::3])\n",
    "#    ax.ticklabel_format(style='plain')\n",
    "#    ax.grid(True)\n",
    "#\n",
    "#plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=\"best_model.keras\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = {i: class_weights[np.where(np.unique(y_train) == i)[0][0]] if i in np.unique(y_train) else 0 for i in range(classifications.num_classes)}\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=num_epochs,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    validation_data=val_ds,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model size: {len(tflite_model)} bytes\")\n",
    "\n",
    "with open(model_h_path, \"w\") as f:\n",
    "    f.write(\"#ifndef _MODELDATA_H_\\n#define _MODELDATA_H_\\n\")\n",
    "    f.write(\"const unsigned char model[] = {\")\n",
    "    f.write(\",\".join(f\"0x{b:02x}\" for b in tflite_model))\n",
    "    f.write(\"};\\n#endif\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
